<?xml version='1.0' encoding='UTF-8' standalone='no'?>
<doxygen xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="compound.xsd" version="1.9.1" xml:lang="en-US">
  <compounddef id="md_pages_demos_demo6" kind="page">
    <compoundname>md_pages_demos_demo6</compoundname>
    <title>1D adv-diff: LSPG with nonlinear manifold projection via MLP</title>
    <briefdescription>
    </briefdescription>
    <detaileddescription>
<para> <mcss:class xmlns:mcss="http://mcss.mosra.cz/doxygen/" mcss:class="m-block m-info" /></para>
<para><simplesect kind="par"><title></title><para>This page describes a demo for a reproductive LSPG ROM applied to a 1D advection-diffusion problem using a nonlinear manifold via a multilayer perceptron (MLP). This demo purposefully focuses on a simple test since the main goal is to demonstrate the steps and the code. The full demo script is <ulink url="https://github.com/Pressio/pressio4py/blob/master/demos/unsteady_default_lspg_advdiff1d_mlp/main.py">here</ulink>.</para>
</simplesect>
</para>
<sect1 id="md_pages_demos_demo6_1autotoc_md145">
<title>Overview</title>
<para>This demo solves the same problem as the one <ulink url="https://pressio.github.io/pressio4py/html/md_pages_demos_demo1.html">here</ulink>, but instead of using POD modes, we show here how to use a nonlinear manifold computed approximated by a neural network. Specifically, we use a MLP with 2 hidden layers of sizes 64 and 200.</para>
<para> <mcss:class xmlns:mcss="http://mcss.mosra.cz/doxygen/" mcss:class="m-block m-warning" /></para>
<para><simplesect kind="par"><title>Important:</title><para>The MLP used in this demo is implemented in PyTorch, thus PyTorch must be installed prior to executing this demo. Look <ulink url="https://pytorch.org/get-started/locally/">here</ulink> for information on how to install PyTorch on your system.</para>
</simplesect>
</para>
</sect1>
<sect1 id="md_pages_demos_demo6_1autotoc_md146">
<title>Main function</title>
<para>The main function of the demo is the following: <programlisting filename=".py"><codeline><highlight class="keywordflow">if</highlight><highlight class="normal"><sp/>__name__<sp/>==<sp/></highlight><highlight class="stringliteral">&quot;__main__&quot;</highlight><highlight class="normal">:</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>logger.initialize(logger.logto.terminal)</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>logger.setVerbosity([logger.loglevel.info])</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">#<sp/>create<sp/>fom<sp/>object</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>fomObj<sp/>=<sp/>AdvDiff1d(nGrid=120,<sp/>adv_coef=2.0)</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">#<sp/>the<sp/>final<sp/>time<sp/>to<sp/>integrate<sp/>to</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>finalTime<sp/>=<sp/>.05</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">#---<sp/>1.<sp/>FOM<sp/>---#</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>fomTimeStepSize<sp/><sp/>=<sp/>1e-5</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>fomNumberOfSteps<sp/>=<sp/>int(finalTime/fomTimeStepSize)</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>sampleEvery<sp/><sp/><sp/><sp/><sp/><sp/>=<sp/>200</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>[fomFinalState,<sp/>snapshots]<sp/>=<sp/>doFom(fomObj,<sp/>fomTimeStepSize,<sp/>fomNumberOfSteps,<sp/>sampleEvery)</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">#---<sp/>2.<sp/>train<sp/>a<sp/>nonlinear<sp/>mapping<sp/>using<sp/>PyTorch<sp/>---#</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">#<sp/>here<sp/>we<sp/>use<sp/>3<sp/>modes,<sp/>change<sp/>this<sp/>to<sp/>try<sp/>different<sp/>modes</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>myNonLinearMapper<sp/>=<sp/>trainMapping(snapshots,<sp/>romSize=3,<sp/>epochs=500)</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">#---<sp/>3.<sp/>LSPG<sp/>ROM<sp/>---#</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>romTimeStepSize<sp/><sp/>=<sp/>3e-4</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>romNumberOfSteps<sp/>=<sp/>int(finalTime/romTimeStepSize)</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>approximatedState<sp/>=<sp/>runLspg(fomObj,<sp/>romTimeStepSize,<sp/>romNumberOfSteps,<sp/>myNonLinearMapper)</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">#<sp/>compute<sp/>l2-error<sp/>between<sp/>fom<sp/>and<sp/>approximate<sp/>state</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>fomNorm<sp/>=<sp/>linalg.norm(fomFinalState)</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>err<sp/>=<sp/>linalg.norm(fomFinalState-approximatedState)</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>print(</highlight><highlight class="stringliteral">&quot;Final<sp/>state<sp/>relative<sp/>l2<sp/>error:<sp/>{}&quot;</highlight><highlight class="normal">.format(err/fomNorm))</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>logger.finalize()</highlight></codeline>
</programlisting></para>
<sect2 id="md_pages_demos_demo6_1autotoc_md147">
<title>1. Run FOM and collect snapshots</title>
<para>This step is the same as described <ulink url="https://pressio.github.io/pressio4py/html/md_pages_demos_demo1.html">here</ulink>,</para>
</sect2>
<sect2 id="md_pages_demos_demo6_1autotoc_md148">
<title>2. Setup and train the nonlinear mapper</title>
<para>It is important to note that while the mapper class below has the API required by pressio4py, it can encapsulate any arbitrary mapping function. In this case we show how to create a MLP-based representation in PyTorch, but one can use any other types of mapping and any other library (e.g., Tensorflow, keras). All of the PyTorch-specific code is encapsulated <ulink url="https://github.com/Pressio/pressio4py/blob/master/demos/unsteady_default_lspg_advdiff1d_mlp/autoencoder_PyTorch.py">here</ulink>. If you prefer Tensorflow/keras, an equivalent implementation is <ulink url="https://github.com/Pressio/pressio4py/blob/master/demos/unsteady_default_lspg_advdiff1d_mlp/autoencoder_keras.py">here</ulink>.</para>
<para>The autoencoder is defined by <programlisting filename=".py"><codeline><highlight class="keyword">class<sp/></highlight><highlight class="normal">myAutoencoder(torch.nn.Module):</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="keyword">def<sp/></highlight><highlight class="normal">__init__(self,<sp/>fomSize,<sp/>romSize=10):</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>super(myAutoencoder,<sp/>self).__init__()</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>self.encoder<sp/>=<sp/>myEncoder(fomSize,<sp/>romSize)</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>self.decoder<sp/>=<sp/>myDecoder(fomSize,<sp/>romSize)</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="keyword">def<sp/></highlight><highlight class="normal">forward(self,<sp/>x):</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>code<sp/>=<sp/>self.encoder(x)</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>x<sp/>=<sp/>self.decoder(code)</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>x,<sp/>code</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="keyword">def<sp/></highlight><highlight class="normal">train(self,<sp/>dataloader,<sp/>optimizer,<sp/>n_epochs,<sp/>loss=torch.nn.MSELoss()):</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>scheduler<sp/>=<sp/>torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,<sp/>factor=0.8,<sp/>min_lr=1e-6)</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">for</highlight><highlight class="normal"><sp/>epoch<sp/></highlight><highlight class="keywordflow">in</highlight><highlight class="normal"><sp/>range(n_epochs):</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/>total_train_loss<sp/>=<sp/>0.0</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">for</highlight><highlight class="normal"><sp/>data,label<sp/></highlight><highlight class="keywordflow">in</highlight><highlight class="normal"><sp/>dataloader:</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>optimizer.zero_grad()</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>output,<sp/>latent<sp/>=<sp/>self.forward(data)</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>loss_size<sp/>=<sp/>loss(output,<sp/>label)</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>loss_size.backward()</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>optimizer.step()</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/>total_train_loss<sp/>+=<sp/>loss_size.item()</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/>scheduler.step(total_train_loss)</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"></highlight><highlight class="keyword">class<sp/></highlight><highlight class="normal">myEncoder(torch.nn.Module):</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="keyword">def<sp/></highlight><highlight class="normal">__init__(self,<sp/>fomSize,<sp/>romSize):</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>super(myEncoder,<sp/>self).__init__()</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>self.fc1<sp/>=<sp/>torch.nn.Linear(fomSize,<sp/>200)</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>self.fc2<sp/>=<sp/>torch.nn.Linear(200,<sp/>64)</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>self.fc3<sp/>=<sp/>torch.nn.Linear(64,<sp/>romSize)</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="keyword">def<sp/></highlight><highlight class="normal">forward(self,<sp/>x):</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>x<sp/>=<sp/>self.fc1(x)</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>x<sp/>=<sp/>F.elu(x)</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>x<sp/>=<sp/>self.fc2(x)</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>x<sp/>=<sp/>F.elu(x)</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>x<sp/>=<sp/>self.fc3(x)</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>x<sp/>=<sp/>F.elu(x)</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>x</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"></highlight><highlight class="keyword">class<sp/></highlight><highlight class="normal">myDecoder(torch.nn.Module):</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="keyword">def<sp/></highlight><highlight class="normal">__init__(self,<sp/>fomSize,<sp/>romSize):</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>super(myDecoder,<sp/>self).__init__()</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>self.romSize_<sp/>=<sp/>romSize</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>self.fomSize_<sp/>=<sp/>fomSize</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>self.fc1<sp/>=<sp/>torch.nn.Linear(romSize,<sp/>64)</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>self.fc2<sp/>=<sp/>torch.nn.Linear(64,<sp/>200)</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>self.fc3<sp/>=<sp/>torch.nn.Linear(200,<sp/>fomSize)</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="keyword">def<sp/></highlight><highlight class="normal">forward(self,<sp/>x):</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>x<sp/>=<sp/>self.fc1(x)</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>x<sp/>=<sp/>F.elu(x)</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>x<sp/>=<sp/>self.fc2(x)</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>x<sp/>=<sp/>F.elu(x)</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>x<sp/>=<sp/>self.fc3(x)</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>x</highlight></codeline>
</programlisting></para>
<para>and is created/trained using <programlisting filename=".py"><codeline><highlight class="keyword">def<sp/></highlight><highlight class="normal">trainMapping(snapshots,<sp/>romSize,<sp/>epochs,<sp/>enable_restart=False):</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>fomSize<sp/>=<sp/>snapshots.shape[0]</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>model<sp/>=<sp/>myAutoencoder(fomSize,<sp/>romSize)</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>optimizer<sp/>=<sp/>optim.AdamW(model.parameters(),<sp/>lr=5e-3)</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">if</highlight><highlight class="normal"><sp/>enable_restart:</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">if</highlight><highlight class="normal"><sp/>pathlib.Path(</highlight><highlight class="stringliteral">&apos;TrainingCheckpoint.tar&apos;</highlight><highlight class="normal">).is_file():</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/>print(</highlight><highlight class="stringliteral">&quot;Loading<sp/>checkpoint&quot;</highlight><highlight class="normal">)</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/>checkpoint<sp/>=<sp/>torch.load(</highlight><highlight class="stringliteral">&apos;TrainingCheckpoint.tar&apos;</highlight><highlight class="normal">)</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/>model.load_state_dict(checkpoint[</highlight><highlight class="stringliteral">&apos;model_state_dict&apos;</highlight><highlight class="normal">])</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/>optimizer.load_state_dict(checkpoint[</highlight><highlight class="stringliteral">&apos;optimizer_state_dict&apos;</highlight><highlight class="normal">])</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>samples<sp/>=<sp/>torch.utils.data.TensorDataset(torch.Tensor(snapshots.T),<sp/>torch.Tensor(snapshots.T))</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>loader<sp/>=<sp/>torch.utils.data.DataLoader(samples,<sp/>batch_size=500,<sp/>shuffle=</highlight><highlight class="keyword">True</highlight><highlight class="normal">)</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>model.train(loader,<sp/>optimizer,<sp/>n_epochs=epochs)</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">if</highlight><highlight class="normal"><sp/>enable_restart:</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>torch.save({</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="stringliteral">&apos;model_state_dict&apos;</highlight><highlight class="normal">:<sp/>model.state_dict(),</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="stringliteral">&apos;optimizer_state_dict&apos;</highlight><highlight class="normal">:<sp/>optimizer.state_dict()</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>},</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="stringliteral">&apos;TrainingCheckpoint.tar&apos;</highlight><highlight class="normal">)</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>MyMapper(model.decoder,<sp/>model.encoder)<sp/></highlight></codeline>
</programlisting></para>
<para>This is all wrapped in a mapper class which conforms to the API required by Pressio <programlisting filename=".py"><codeline><highlight class="keyword">class<sp/></highlight><highlight class="normal">MyMapper:</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="keyword">def<sp/></highlight><highlight class="normal">__init__(self,<sp/>decoderObj,<sp/>encoderObj):</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>self.decoder_<sp/>=<sp/>decoderObj</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>self.encoder_<sp/>=<sp/>encoderObj</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>self.numModes_<sp/>=<sp/>decoderObj.romSize_</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>fomSize<sp/>=<sp/>decoderObj.fomSize_</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>self.fomState0<sp/>=<sp/>np.zeros(fomSize)</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>self.fomState1<sp/>=<sp/>np.zeros(fomSize)</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="comment">#<sp/>attention:<sp/>the<sp/>jacobian<sp/>of<sp/>the<sp/>mapping<sp/>must<sp/>be<sp/>column-major<sp/>oder</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="comment">#<sp/>so<sp/>that<sp/>pressio<sp/>can<sp/>view<sp/>it<sp/>without<sp/>deep<sp/>copying<sp/>it,<sp/>this<sp/>enables</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="comment">#<sp/>to<sp/>keep<sp/>only<sp/>one<sp/>jacobian<sp/>object<sp/>around<sp/>and<sp/>to<sp/>call<sp/>the<sp/>update</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="comment">#<sp/>method<sp/>below<sp/>correctly</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>self.jacobian_<sp/>=<sp/>np.zeros((fomSize,self.numModes_),<sp/>order=</highlight><highlight class="stringliteral">&apos;F&apos;</highlight><highlight class="normal">)</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="keyword">def<sp/></highlight><highlight class="normal">jacobian(self):<sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>self.jacobian_</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="keyword">def<sp/></highlight><highlight class="normal">applyMapping(self,<sp/>romState,<sp/>fomState):</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>fomState[:]<sp/>=<sp/>self.decoder_(torch.Tensor(romState)).detach().numpy()</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="keyword">def<sp/></highlight><highlight class="normal">applyInverseMapping(self,<sp/>fomState):</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>romState<sp/>=<sp/>np.zeros(self.numModes_)</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>romState[:]<sp/>=<sp/>self.encoder_(torch.Tensor(fomState)).detach()[:]</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>romState</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="keyword">def<sp/></highlight><highlight class="normal">updateJacobian(self,<sp/>romState):</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>self.updateJacobianFD(romState)</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="keyword">def<sp/></highlight><highlight class="normal">updateJacobianFD(self,<sp/>romState):</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="comment">#<sp/>finite<sp/>difference<sp/>to<sp/>approximate<sp/>jacobian<sp/>of<sp/>the<sp/>mapping</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>romStateLocal<sp/>=<sp/>romState.copy()</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>self.applyMapping(romStateLocal,self.fomState0)</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>eps<sp/>=<sp/>0.001</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">for</highlight><highlight class="normal"><sp/>i<sp/></highlight><highlight class="keywordflow">in</highlight><highlight class="normal"><sp/>range(self.numModes_):</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>romStateLocal[i]<sp/>+=<sp/>eps</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>self.applyMapping(romStateLocal,<sp/>self.fomState1)</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>self.jacobian_[:,i]<sp/>=<sp/>(self.fomState1<sp/>-<sp/>self.fomState0)<sp/>/<sp/>eps</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>romStateLocal[i]<sp/>-=<sp/>eps</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="keyword">def<sp/></highlight><highlight class="normal">updateJacobianExact(self,<sp/>romState):</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="comment">#<sp/>use<sp/>pytorch<sp/>autodifferentiation<sp/>to<sp/>compute<sp/>jacobian<sp/>of<sp/>the<sp/>mapping</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="comment">#<sp/>slower<sp/>than<sp/>finite<sp/>difference<sp/>currently</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>J<sp/>=<sp/>torch.autograd.functional.jacobian(self.decoder_,<sp/>torch.Tensor(romState))</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>self.jacobian_[:,:]<sp/>=<sp/>J.detach()[:,:]</highlight></codeline>
</programlisting></para>
<para> <mcss:class xmlns:mcss="http://mcss.mosra.cz/doxygen/" mcss:class="m-block m-warning" /></para>
<para><simplesect kind="par"><title>Important:</title><para>when creating an arbitrary mapping (as in the class above), the jacobian matrix <bold>must</bold> be column-major oder so that pressio can reference it without deep copying it. This not only reduces the memory footprint since it allows to keep only one jacobian object around but also it is fundamental for the update method below correctly.</para>
</simplesect>
</para>
</sect2>
<sect2 id="md_pages_demos_demo6_1autotoc_md149">
<title>3. Construct and run LSPG</title>
<para><programlisting filename=".py"><codeline><highlight class="keyword">def<sp/></highlight><highlight class="normal">runLspg(fomObj,<sp/>dt,<sp/>nsteps,<sp/>customMapper):</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">#<sp/>this<sp/>is<sp/>an<sp/>auxiliary<sp/>class<sp/>that<sp/>can<sp/>be<sp/>passed<sp/>to<sp/>solve</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">#<sp/>LSPG<sp/>to<sp/>monitor<sp/>the<sp/>rom<sp/>state.</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="keyword">class<sp/></highlight><highlight class="normal">RomStateObserver:</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keyword">def<sp/></highlight><highlight class="normal">__call__(self,<sp/>timeStep,<sp/>time,<sp/>state):<sp/></highlight><highlight class="keyword">pass</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">#<sp/>this<sp/>linear<sp/>solver<sp/>is<sp/>used<sp/>at<sp/>each<sp/>gauss-newton<sp/>iteration</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="keyword">class<sp/></highlight><highlight class="normal">MyLinSolver:</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keyword">def<sp/></highlight><highlight class="normal">solve(self,<sp/>A,b,x):</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/>lumat,<sp/>piv,<sp/>info<sp/>=<sp/>linalg.lapack.dgetrf(A,<sp/>overwrite_a=</highlight><highlight class="keyword">True</highlight><highlight class="normal">)</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/>x[:],<sp/>info<sp/>=<sp/>linalg.lapack.dgetrs(lumat,<sp/>piv,<sp/>b,<sp/>0,<sp/>0)</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">#----------------------------------------</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">#<sp/>create<sp/>a<sp/>custom<sp/>decoder<sp/>using<sp/>the<sp/>mapper<sp/>passed<sp/>as<sp/>argument</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>customDecoder<sp/>=<sp/>rom.Decoder(customMapper,<sp/></highlight><highlight class="stringliteral">&quot;MyMapper&quot;</highlight><highlight class="normal">)</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">#<sp/>fom<sp/>reference<sp/>state:<sp/>here<sp/>it<sp/>is<sp/>zero</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>fomReferenceState<sp/>=<sp/>np.zeros(fomObj.nGrid)</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">#<sp/>create<sp/>ROM<sp/>state<sp/>by<sp/>projecting<sp/>the<sp/>fom<sp/>initial<sp/>condition</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>fomInitialState<sp/>=<sp/>fomObj.u0.copy()</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>romState<sp/>=<sp/>customMapper.applyInverseMapping(fomInitialState)</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">#<sp/>create<sp/>LSPG<sp/>problem</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>scheme<sp/>=<sp/>ode.stepscheme.BDF1</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>problem<sp/>=<sp/>rom.lspg.unsteady.DefaultProblem(scheme,<sp/>fomObj,<sp/>customDecoder,<sp/>romState,<sp/>fomReferenceState)</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>stepper<sp/>=<sp/>problem.stepper()</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">#<sp/>create<sp/>the<sp/>Gauss-Newton<sp/>solver</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>nonLinSolver<sp/>=<sp/>solvers.create_gauss_newton(stepper,<sp/>romState,<sp/>MyLinSolver())</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">#<sp/>set<sp/>tolerance<sp/>and<sp/>convergence<sp/>criteria</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>nlsTol,<sp/>nlsMaxIt<sp/>=<sp/>1e-7,<sp/>10</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>nonLinSolver.setMaxIterations(nlsMaxIt)</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>nonLinSolver.setStoppingCriterion(solvers.stop.WhenCorrectionAbsoluteNormBelowTolerance)</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">#<sp/>create<sp/>object<sp/>to<sp/>monitor<sp/>the<sp/>romState<sp/>at<sp/>every<sp/>iteration</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>myObs<sp/>=<sp/>RomStateObserver()</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">#<sp/>solve<sp/>problem</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>ode.advance_n_steps_and_observe(stepper,<sp/>romState,<sp/>0.,<sp/>dt,<sp/>nsteps,<sp/>myObs,<sp/>nonLinSolver)</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">#<sp/>after<sp/>we<sp/>are<sp/>done,<sp/>use<sp/>the<sp/>reconstructor<sp/>object<sp/>to<sp/>reconstruct<sp/>the<sp/>fom<sp/>state</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">#<sp/>get<sp/>the<sp/>reconstructor<sp/>object:<sp/>this<sp/>allows<sp/>to<sp/>map<sp/>romState<sp/>to<sp/>fomState</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>fomRecon<sp/>=<sp/>problem.fomStateReconstructor()</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>fomRecon(romState)</highlight></codeline>
</programlisting></para>
</sect2>
</sect1>
<sect1 id="md_pages_demos_demo6_1autotoc_md150">
<title>Results</title>
<para>If everything works fine, the following plot shows the result. <image type="html" name="demo6.png"></image>
 </para>
</sect1>
    </detaileddescription>
    <location file="pages/demos/demo6.md"/>
  </compounddef>
</doxygen>
